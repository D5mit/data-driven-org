{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI 2 Trust Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas module \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count answers\n",
    "def create_list(ans_column):\n",
    "    ans_dist = [ sum(df[ans_column] == 1),\n",
    "                 sum(df[ans_column] == 2),\n",
    "                 sum(df[ans_column] == 3),\n",
    "                 sum(df[ans_column] == 4),\n",
    "                 sum(df[ans_column] == 5), ]\n",
    "    return ans_dist\n",
    "\n",
    "# count answers two categories\n",
    "def create_list_bi(ans_column):\n",
    "    ans_dist = [ sum(df[ans_column] == 1) +  sum(df[ans_column] == 2) +  sum(df[ans_column] == 3),\n",
    "                 sum(df[ans_column] == 4) +  sum(df[ans_column] == 5), ]\n",
    "    return ans_dist\n",
    "\n",
    "\n",
    "# create survey results chart\n",
    "def survey(results, category_names):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        A mapping from question labels to a list of answers per category.\n",
    "        It is assumed all lists contain the same number of entries and that\n",
    "        it matches the length of *category_names*.\n",
    "    category_names : list of str\n",
    "        The category labels.\n",
    "    \"\"\"\n",
    "    labels = list(results.keys())\n",
    "    data = np.array(list(results.values()))\n",
    "    data_cum = data.cumsum(axis=1)\n",
    "    category_colors = plt.get_cmap('RdYlGn')(\n",
    "        np.linspace(0.15, 0.85, data.shape[1]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_xlim(0, np.sum(data, axis=1).max())\n",
    "\n",
    "    for i, (colname, color) in enumerate(zip(category_names, category_colors)):\n",
    "        widths = data[:, i]\n",
    "        starts = data_cum[:, i] - widths\n",
    "        rects = ax.barh(labels, widths, left=starts, height=0.5,\n",
    "                        label=colname, color=color)\n",
    "\n",
    "        r, g, b, _ = color\n",
    "        text_color = 'white' if r * g * b < 0.5 else 'darkgrey'\n",
    "        ax.bar_label(rects, label_type='center', color=text_color)\n",
    "    ax.legend(ncol=len(category_names), bbox_to_anchor=(0, 1),\n",
    "              loc='lower left', fontsize='small')\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "# functions used in notebook\n",
    "def check_pvalue(p_df, p_imetric, p_di_isnb, p_di_nonb, p_hx):\n",
    "    \"\"\"\n",
    "    This function takes the game results and identifies the last move of the game that lead to a loss and\n",
    "    then reverse the move.  The output is a dataframe containing moves that needs to be improved.\n",
    "    This will form part of the training data.\n",
    "    \n",
    "    parameters: inr_of_game (int) number of games\n",
    "                inr_of_files (int) number of files in which the games will be split up\n",
    "                player_1 (char1) player x type, U for untrained, S for trained and L for trained further\n",
    "                player_2 (char1) player o type, U for untrained, S for trained and L for trained further\n",
    "                i_print_board (boolean) True if the board state should be printed\n",
    "    return: filename (string) name of file that was created        \n",
    "    \"\"\"\n",
    "    \n",
    "    # create new dataframe that will be used\n",
    "    p_df_new = p_df[[p_imetric]].copy()\n",
    "    p_df_new.loc[:, 'isnb'] = p_df_new.loc[:, p_imetric]\n",
    "    p_df_new.loc[:, 'nonb'] = p_df_new.loc[:, p_imetric]\n",
    "\n",
    "    # code data\n",
    "    p_df_new = p_df_new.replace({\"isnb\": p_di_isnb})\n",
    "    p_df_new = p_df_new.replace({\"nonb\": p_di_nonb})\n",
    "\n",
    "    # drop na\n",
    "    p_df_new = p_df_new.dropna()\n",
    "\n",
    "    # 1 calcualte observed differences\n",
    "    p_obs_diff = p_df_new['isnb'].sum() - p_df_new['nonb'].sum()\n",
    "\n",
    "    # 2 - create sampling distribution of difference between isnb and nonb\n",
    "    diffs = []\n",
    "    for _ in range(10000):\n",
    "        df_sample = p_df_new.sample(p_df_new.shape[0], replace = True)\n",
    "        sample_diff = df_sample['isnb'].sum() - df_sample['nonb'].sum()\n",
    "\n",
    "        diffs.append(sample_diff)    \n",
    "        \n",
    "    diffs = np.array(diffs)        \n",
    "    \n",
    "    # 3 - simulate distribution under the null hypothesis\n",
    "    null_vals = np.random.normal(0, diffs.std(), diffs.size)    \n",
    "    \n",
    "    # plot null distribution\n",
    "    plt.hist(null_vals);\n",
    "\n",
    "    # plot line for observed statistic\n",
    "    plt.axvline(x=p_obs_diff, color='red');    \n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # compute p value\n",
    "    p_value = (null_vals > p_obs_diff).mean()\n",
    "    \n",
    "    print(p_hx, p_imetric)    \n",
    "    print('p-value:', p_value)\n",
    "    if p_value < 0.001:      \n",
    "        print('reject H0')\n",
    "    else:\n",
    "        print('fail to reject H0')\n",
    "\n",
    "    print('----')    \n",
    "    print('')       \n",
    "    \n",
    "    return p_value\n",
    "\n",
    "\n",
    "# function to create word cloud \n",
    "def create_wordcloud(df_cloud, i_column):\n",
    "    \"\"\" \n",
    "    creates a wordcload based on a dataframe column as input. \n",
    "\n",
    "    Parameters: \n",
    "    df_main (data frame): dataframe containing data\n",
    "    i_column (string): Column \n",
    "\n",
    "    Returns: \n",
    "    int: wordcloud\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the text \n",
    "    df_text = df_cloud.dropna(subset=[i_column]) \n",
    "    df_text = df_text[i_column]\n",
    "\n",
    "    # take dataframe and put in text\n",
    "    text = \" \".join(itext for itext in df_text)\n",
    "\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update([\"AI\", \"system\", \"organization\", 'application', 'one',  \"will\", \"need\", 'without', 'something', 'make',\n",
    "                       'based', 'stay', 'add', 'day', 'still', 'ie', 'way', 'made'])\n",
    "\n",
    "    # Create and generate a word cloud image:\n",
    "    wordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=30, background_color=\"white\").generate(text)\n",
    "\n",
    "    # Display the generated image:\n",
    "    plt.figure(figsize=(30, 20))    \n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\DANIES~1\\AppData\\Local\\Temp/ipykernel_15396/2748189377.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# import header file\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_excel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'http://www.d5mit.co.za/artifacts/output.xlsx'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'ai_transparency_to_trust'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnotna\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\data-driven-org\\lib\\site-packages\\pandas\\util\\_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m                 )\n\u001B[1;32m--> 311\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    312\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\data-driven-org\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001B[0m in \u001B[0;36mread_excel\u001B[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001B[0m\n\u001B[0;32m    362\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mExcelFile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    363\u001B[0m         \u001B[0mshould_close\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 364\u001B[1;33m         \u001B[0mio\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mExcelFile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstorage_options\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mengine\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    365\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    366\u001B[0m         raise ValueError(\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\data-driven-org\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, path_or_buffer, engine, storage_options)\u001B[0m\n\u001B[0;32m   1231\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstorage_options\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1232\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1233\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engines\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_io\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstorage_options\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1234\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1235\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__fspath__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\data-driven-org\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, filepath_or_buffer, storage_options)\u001B[0m\n\u001B[0;32m    519\u001B[0m             \u001B[0mpassed\u001B[0m \u001B[0mto\u001B[0m \u001B[0mfsspec\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mappropriate\u001B[0m \u001B[0mURLs\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0msee\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0m_get_filepath_or_buffer\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    520\u001B[0m         \"\"\"\n\u001B[1;32m--> 521\u001B[1;33m         \u001B[0mimport_optional_dependency\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"openpyxl\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    522\u001B[0m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstorage_options\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    523\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\data-driven-org\\lib\\site-packages\\pandas\\compat\\_optional.py\u001B[0m in \u001B[0;36mimport_optional_dependency\u001B[1;34m(name, extra, errors, min_version)\u001B[0m\n\u001B[0;32m    116\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mImportError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    117\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0merrors\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"raise\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 118\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mImportError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    119\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    120\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl."
     ]
    }
   ],
   "source": [
    "# import header file\n",
    "df = pd.read_excel('http://www.d5mit.co.za/artifacts/output.xlsx')\n",
    "\n",
    "df = df[df['ai_transparency_to_trust'].notna()]\n",
    "\n",
    "df.drop('email', axis='columns', inplace=True)\n",
    "df.drop('first_name', axis='columns', inplace=True)\n",
    "df.drop('last_name', axis='columns', inplace=True)\n",
    "df.drop('custom_data', axis='columns', inplace=True)\n",
    "\n",
    "df = df[df['ai_fat_to_trust'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "The questions are in the context of implementing AI applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of answers\n",
    "category_names = ['Not at all important','Not so important','Somewhat important','Very important','Extremely important']\n",
    "\n",
    "results = {\n",
    "    'Transparency to Trust': create_list('ai_transparency_to_trust'),\n",
    "    'Accountability to Trust': create_list('ai_accountability_to_trust'),\n",
    "    'Fairness to Trust': create_list('ai_fairness_to_trust'),\n",
    "    'Trust to Adoption': create_list('ai_trust_to_adoption'),\n",
    "    'FAT to Trust': create_list('ai_fat_to_trust'),\n",
    "}\n",
    "\n",
    "survey(results, category_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating H1-H5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>H1: \n",
    "ai_transparency_to_trust </strong>\n",
    "<br>Question: \n",
    "From a product owner perspective, do you think the transparency of an AI application's logic is important/beneficial for trustworthiness? Transparency is, for example, the explainability of the machine learning decision/predictive process.\n",
    "<br><br>\n",
    "<strong>H2: \n",
    "ai_accountability_to_trust </strong>\n",
    "<br>Question: \n",
    "From a product owner perspective, do you think assigning accountability of a AI application's actions is important/beneficial for trustworthiness? Accountability would be, for example, who is accountable when an autonomous vehicle crashes.\n",
    "<br><br>\n",
    "<strong>H3: \n",
    "ai_fairness_to_trust</strong>\n",
    "<br>Question: \n",
    "From a product owner perspective, do you think the fairness of the AI application's actions is important for trustworthiness. For example, demographic biases in the automation of credit vetting.\n",
    "<br><br>\n",
    "<strong>H4: \n",
    "ai_trust_to_adoption </strong>\n",
    "<br>Question: \n",
    "From a product owner perspective, is trustworthiness an important contributor to the adoption of the system?\n",
    "<br><br>\n",
    "<strong>H5: \n",
    "ai_fat_to_trust </strong>\n",
    "<br>Question: \n",
    "Ensuring that AI applications are fair, accountable and transparent, will lead to more trust.\n",
    "<br><br>\n",
    "\n",
    "<strong>Answeres mapped into groups:</strong>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Value Text</th>\n",
    "        <th>Number</th>\n",
    "        <th>di_isnb</th>   \n",
    "        <th>di_nonb</th>         \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Extremely important</td>\n",
    "        <td>5</td>\n",
    "        <td>1</td>        \n",
    "        <td>0</td>                \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Very important</td>\n",
    "        <td>4</td>\n",
    "        <td>1</td>        \n",
    "        <td>0</td>                        \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Somewhat important</td>\n",
    "        <td>3</td>\n",
    "        <td>0</td>        \n",
    "        <td>1</td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Not so important</td>\n",
    "        <td>2</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>        \n",
    "    </tr>    \n",
    "    <tr>\n",
    "        <td>Not at all important</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>  \n",
    "        <td>1</td>        \n",
    "    </tr>    \n",
    "</table>    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "<i>Other</i><br>\n",
    "<strong>H4b: \n",
    "ai_lack_trust_low_adoption</strong>\n",
    "<br>Question: \n",
    "A lack of trust in the process around an AI system will lead to a lower level of adoption of the AI application.\n",
    "<br><br>\n",
    "<strong>H6: \n",
    "ai_must_be_trans_expl</strong>\n",
    "<br>Question: \n",
    "Organisations need to ensure that the AI applications that they develop are transparent (allow explainability) in their actions.\n",
    "<br><br>\n",
    "<strong>What other ethical considerations: \n",
    "ai_ethical_considerations</strong>\n",
    "<br>Question: \n",
    "If any, what ethical considerations other than fairness, transparency and accountability should organisations have when they develop AI applications?\n",
    "<br><br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/TOEFAT_H.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of answers\n",
    "category_names = ['Not at all important, Not so important, Somewhat important','Very important, Extremely important']\n",
    "\n",
    "results = {\n",
    "    'H1: Transparency to Trust': create_list_bi('ai_transparency_to_trust'),\n",
    "    'H2: Accountability to Trust': create_list_bi('ai_accountability_to_trust'),\n",
    "    'H3: Fairness to Trust': create_list_bi('ai_fairness_to_trust'),\n",
    "    'H4: Trust to Adoption': create_list_bi('ai_trust_to_adoption'),\n",
    "    'H5: FAT to Trust': create_list_bi('ai_fat_to_trust'),\n",
    "}\n",
    "\n",
    "survey(results, category_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H1: ai_transparency_to_trust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Hypothesis</strong><br>\n",
    "H0: isnb - nonb = 0 <br>\n",
    "H1: isnb - nonb > 0 <br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set H1 field\n",
    "imetric = 'ai_transparency_to_trust'\n",
    "\n",
    "# create dictionary with mappings\n",
    "di_isnb = {5: 1, 4: 1, 3: 0, 2: 0, 1: 0}\n",
    "di_nonb = {5: 0, 4: 0, 3: 1, 2: 1, 1: 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = df[[imetric]].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe that will be used\n",
    "df_new = df[[imetric]].copy()\n",
    "df_new.loc[:, 'isnb'] = df_new.loc[:, imetric]\n",
    "df_new.loc[:, 'nonb'] = df_new.loc[:, imetric]\n",
    "\n",
    "# code data\n",
    "df_new = df_new.replace({\"isnb\": di_isnb})\n",
    "df_new = df_new.replace({\"nonb\": di_nonb})\n",
    "\n",
    "# drop na\n",
    "df_new = df_new.dropna()\n",
    "\n",
    "# output data\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = df_new[['isnb']].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Computed the observed difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_diff = df_new['isnb'].sum() - df_new['nonb'].sum()\n",
    "print(obs_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Simulate the sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sampling distribution of difference between isnb and nonb\n",
    "diffs = []\n",
    "for _ in range(10000):\n",
    "    df_sample = df_new.sample(df_new.shape[0], replace = True)\n",
    "    sample_diff = df_sample['isnb'].sum() - df_sample['nonb'].sum()\n",
    "\n",
    "    diffs.append(sample_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std of simulated differences\n",
    "diffs = np.array(diffs)\n",
    "diffs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sampling distribution\n",
    "plt.hist(diffs);\n",
    "plt.axvline(x=obs_diff, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Simulate the distribution under the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate distribution under the null hypothesis\n",
    "null_vals = np.random.normal(0, diffs.std(), diffs.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot null distribution\n",
    "plt.hist(null_vals);\n",
    "\n",
    "# plot line for observed statistic\n",
    "plt.axvline(x=obs_diff, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Compute the p-value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute p value\n",
    "p_value = (null_vals > obs_diff).mean()\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P < 0.05 -> statistically significant\n",
    "# P < 0.01 -> \n",
    "# P < 0.001 -> statistically highly significant\n",
    "\n",
    "if p_value < 0.001:      #  statistically highly significant as P < 0.001 \n",
    "    print('reject H0')\n",
    "else:\n",
    "    print('fail to reject H0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate P-value for all H and test H0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H1: ai_transparency_to_trust\n",
    "imetric = 'ai_transparency_to_trust'\n",
    "hx = 'H1: '\n",
    "p_value = check_pvalue(df, imetric, di_isnb, di_nonb, hx)\n",
    " \n",
    "# H2: ai_accountability_to_trust\n",
    "imetric = 'ai_accountability_to_trust'\n",
    "hx = 'H2: '\n",
    "p_value = check_pvalue(df, imetric, di_isnb, di_nonb, hx)\n",
    "\n",
    "# H3: ai_fairness_to_trust\n",
    "imetric = 'ai_fairness_to_trust'\n",
    "hx = 'H3: '\n",
    "p_value = check_pvalue(df, imetric, di_isnb, di_nonb, hx)\n",
    "\n",
    "# H4: ai_trust_to_adoption\n",
    "imetric = 'ai_trust_to_adoption'\n",
    "hx = 'H4: '\n",
    "p_value = check_pvalue(df, imetric, di_isnb, di_nonb, hx)\n",
    "\n",
    "# H5: ai_fat_to_trust\n",
    "imetric = 'ai_fat_to_trust'\n",
    "hx = 'H5: '\n",
    "p_value = check_pvalue(df, imetric, di_isnb, di_nonb, hx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Ethical considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(df, 'ai_ethical_considerations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-f5b8e07e",
   "language": "python",
   "display_name": "PyCharm (data-driven-org)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}